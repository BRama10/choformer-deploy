task: "train" # train, test, benchmarking (to be set up later)
dna_fm_id: "zhihan1996/DNABERT-2-117M" # pretrained dna foundation model id for huggingface

decoder_model:
  protein_embedding_size: 320
  decoder_size: 512
  dna_seq_len: 1000 # max number of dna tokens for truncation/padding
  dna_model_path: "zhihan1996/DNABERT-2-117M"
  layers: 6
  heads: 8
  dropout: 0.1

decoder_hparams:
  beta1: 0.9
  beta2: 0.999
  num_epochs: 10
  lr: 5e-4


lstm_model:
  input_size: 768
  hidden_size: 384
  num_layers: 2

lstm_train:
  batch_size: 16
  n_epochs: 10
  grad_accum_iter: 4
  grad_clip: 5.0
  epochs_per_val: 1
  learning_rate: 3e-3
  weight_decay: 1e-3
  beta_1: 0.9
  beta_2: 0.999
  ckpt_path: ""

data:
  train_data_path: "/Users/Shrey/Shrey_Work/Duke/!Research/ChoFormer/choformer/train.csv"
  val_data_path: "/Users/Shrey/Shrey_Work/Duke/!Research/ChoFormer/choformer/val.csv"
  test_data_path: "/Users/Shrey/Shrey_Work/Duke/!Research/ChoFormer/choformer/test.csv"
  batch_size: 16
  num_workers: 8

log:
  project: explstm
  name: test_run
  ckpt_path: "./ckpts"